import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=10000, n_features=9, class_sep=4, n_informative=4, random_state = 18)
X=pd.DataFrame(X)
X.columns=["öz1","öz2","öz3","öz4","öz5","öz6","öz7","öz8","öz9"]
X
y=pd.DataFrame(y)
y.columns=["Target"]
y
data=pd.merge(X,y, left_index=True, right_index=True)
data.duplicated().sum()
data.isna().sum()

import seaborn as sns
plt.figure(figsize=(16, 8))
sns.distplot(data["öz1"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz2"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz3"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz4"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz5"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz6"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz7"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz8"])

plt.figure(figsize=(16, 8))
sns.distplot(data["öz9"])

sns.pairplot(X)

data.corr()

# Correlation of the features
import seaborn as sns
corr = data.corr()

plt.figure(figsize=(14, 14))
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True, annot = True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_ylim(len(corr)+0.5, -0.5);

data=data.drop("öz4", axis=1)
data=data.drop("öz5", axis=1)
X=X.drop("öz4", axis=1)
X=X.drop("öz5", axis=1)

# Outlier detection with Z-Score
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(X))
z
len(np.where(z > 3)[0])
outliers = list(set(np.where(z > 3)[0]))
df = data.drop(outliers,axis = 0).reset_index(drop = False)
display(df)

X=df.drop("Target",axis=1)
X
X=X.drop("index",axis=1)
X

from sklearn.preprocessing import StandardScaler, MinMaxScaler
X_scaled = MinMaxScaler().fit_transform(X)
X=X_scaled
X

X=pd.DataFrame(X, columns=["öz1","öz2","öz3","öz6","öz7","öz8","öz9"])
y=df["Target"]
y
y.value_counts()
X.corr()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

# Karar Ağacını içe aktarın, farklı hiperparametreleri deneyerek algoritmayı ayarlayın. (hyperpara)
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(max_depth=4, random_state=42)
clf.fit(X_train,y_train)
print("Accuracy of train:",clf.score(X_train,y_train))
print("Accuracy of test:",clf.score(X_test,y_test))

#Visualization

import os
from sklearn.tree import export_graphviz
# We need to locate graphiz directory for visualization (after conda)
os.environ["PATH"] += ';' + r'C:\Users\Dell\Anaconda3\Library\bin\graphviz'

import graphviz

dot_data = export_graphviz(clf, out_file=None,
                     filled=True, rounded=True)
graph = graphviz.Source(dot_data)
graph

# Öznitelik önemlerini (feature importances) görselleştirin.
#Feature Importance
plt.figure(figsize=(12, 8))
importance = clf.feature_importances_
sns.barplot(x=importance, y=X.columns)
plt.show()

# Hata matrisini oluşturun ve accuracy, recall, precision ve f1-score değerlerini hesaplayın.
# Classification Report
from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score
pred = clf.predict(X_test)
print(classification_report(y_test,pred))

# Metrics
print("Precision = {}".format(precision_score(y_test, pred, average='macro')))
print("Recall = {}".format(recall_score(y_test, pred, average='macro')))
print("Accuracy = {}".format(accuracy_score(y_test, pred)))
print("F1 Score = {}".format(f1_score(y_test, pred,average='macro')))

# XGBoostClassifier'ı içe aktarın, farklı hiperparametreleri deneyerek algoritmayı ayarlayın.

import xgboost as xgb
dmatrix_train = xgb.DMatrix(data=X_train, label=y_train)
dmatrix_test = xgb.DMatrix(data=X_test, label=y_test)
dmatrix_train

param = {'max_depth':3, 
         'eta':1, 
         'objective':'binary:hinge'}
         
num_round = 1
model = xgb.train(param, dmatrix_train, num_round)

preds = model.predict(dmatrix_test)
predictions = [round(value) for value in preds]
preds[:10]

best_preds = np.asarray([np.argmax(line) for line in preds])
print("Precision = {}".format(precision_score(y_test, best_preds, average='macro')))
print("Recall = {}".format(recall_score(y_test, best_preds, average='macro')))
print("Accuracy = {}".format(accuracy_score(y_test, best_preds)))

from sklearn.metrics import confusion_matrix
categories={0,1}
plt.figure(figsize=(12, 8))
cm = confusion_matrix(y_test, best_preds)
ax = sns.heatmap(cm, square=True, annot=True, cbar=False)
ax.xaxis.set_ticklabels(categories, fontsize = 12)
ax.yaxis.set_ticklabels(categories, fontsize = 12, rotation=0)
ax.set_xlabel('Predicted Labels',fontsize = 15)
ax.set_ylabel('True Labels',fontsize = 15)
plt.show()

# Hata matrisini oluşturun ve accuracy, recall, precision ve f1-score değerlerini hesaplayın.
# Classification Report
from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score
pred = model.predict(X_test)
print(classification_report(y_test,pred))

# Metrics
print("Precision = {}".format(precision_score(y_test, pred, average='macro')))
print("Recall = {}".format(recall_score(y_test, pred, average='macro')))
print("Accuracy = {}".format(accuracy_score(y_test, pred)))
print("F1 Score = {}".format(f1_score(y_test, pred,average='macro')))

## Hyperparameter Tuning

from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  

param_dict = {
    'max_depth':range(3,10,2),
    'min_child_weight':range(1,6,2),
    'learning_rate': [0.00001,0.001,0.01,0.1,1,2],
    'n_estimators': [10,190,200,210,500,1000,2000]
    
}

xgc = XGBClassifier(booster='gbtree', learning_rate =0.01, n_estimators=200, max_depth=5,
 min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
 objective= 'multi:softprob', nthread=4, scale_pos_weight=1, seed=27)

clf = GridSearchCV(xgc,param_dict,cv=3, n_jobs = -1).fit(X_train,y_train)

print("Tuned: {}".format(clf.best_params_)) 
print("Mean of the cv scores is {:.6f}".format(clf.best_score_))
print("Train Score {:.6f}".format(clf.score(X_train,y_train)))
print("Test Score {:.6f}".format(clf.score(X_test,y_test)))
print("Seconds used for refitting the best model on the train dataset: {:.6f}".format(clf.refit_time_))

plt.figure(figsize=(12, 8))

xgb_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, xgb_pred)
ax = sns.heatmap(cm, square=True, annot=True, cbar=False)
ax.xaxis.set_ticklabels(categories, fontsize = 12)
ax.yaxis.set_ticklabels(categories, fontsize = 12, rotation=0)
ax.set_xlabel('Predicted Labels',fontsize = 15)
ax.set_ylabel('True Labels',fontsize = 15)
plt.show()